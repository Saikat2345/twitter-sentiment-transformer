{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kos8LoHBrgMP"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqcMmFxFtG0o"
      },
      "outputs": [],
      "source": [
        "#for text embedding\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L2DgK-0r48i"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIJGwMLzsLgn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/twitter-entity-sentiment-analysis/twitter_training.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2adegzmJsZd0"
      },
      "outputs": [],
      "source": [
        "df['Positive'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bou37L13sd19"
      },
      "outputs": [],
      "source": [
        "df=df.drop(['2401','Borderlands'],axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWTjmKQDsngF"
      },
      "outputs": [],
      "source": [
        "df=df.rename(columns={\n",
        "    'Positive':'sentiment',\n",
        "    'im getting on borderlands and i will murder you all ,':'comment'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqyuOPBtsrLd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ZR7kBPuclQ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkVf6gnPsv4B"
      },
      "outputs": [],
      "source": [
        "level_encoder=LabelEncoder()\n",
        "df['sentiment']=level_encoder.fit_transform(df['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebWxYMcss16X"
      },
      "outputs": [],
      "source": [
        "df['comment'] = df['comment'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjGmG0ArtU6I"
      },
      "outputs": [],
      "source": [
        "print(df['comment'].apply(type).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShMyYF5CtXRZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "comments_list = df['comment'].tolist()\n",
        "comment_embeddings = model.encode(comments_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJUPFdb0tesE"
      },
      "outputs": [],
      "source": [
        "df['embedding'] = list(comment_embeddings)\n",
        "df=df.drop(['comment'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test=train_test_split(df['embedding'],df['sentiment'],random_state=42,test_size=0.2)"
      ],
      "metadata": {
        "id": "UQSYMFQTjEN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_stacked = np.array(x_train.tolist())\n",
        "X_test_stacked = np.array(x_test.tolist())"
      ],
      "metadata": {
        "id": "wvnz-w0IjEDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train_stacked, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_stacked, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "78hrbw01jDsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "QV0w6cB6jgZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    \"\"\"Calculates the scaled dot-product attention.\"\"\"\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        # Check to prevent architectural errors before model creation\n",
        "        assert d_model % num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # Now we can safely reshape\n",
        "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
        "\n",
        "        out = self.linear_layer(values)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ST39CxXUjgWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    \"\"\"Layer Normalization module.\"\"\"\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "QGxEEowNjt_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward network.\"\"\"\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qDku8JOOjt7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"A single layer of the Transformer Encoder.\"\"\"\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        residual_x = x\n",
        "        x = self.attention(x, mask=mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gEP3_uubjt5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"The full Transformer Encoder.\"\"\"\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "K7UTWqiyj9n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "        self.classifier_head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input for the encoder: (batch_size, d_model) -> (batch_size, 1, d_model)\n",
        "        x = x.unsqueeze(1)\n",
        "        encoder_output = self.encoder(x)\n",
        "        # Squeeze output for the classifier head: (batch_size, 1, d_model) -> (batch_size, d_model)\n",
        "        encoder_output = encoder_output.squeeze(1)\n",
        "        logits = self.classifier_head(encoder_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "P1RKgryjj9b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "UxP2C7zQkCZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 384  # This MUST match your data's feature dimension\n",
        "NUM_HEADS = 8\n",
        "FFN_HIDDEN = D_MODEL * 4  # A common practice\n",
        "DROP_PROB = 0.1\n",
        "NUM_LAYERS = 6 # Number of EncoderLayers\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LR = 1e-4\n",
        "NUM_CLASSES=4"
      ],
      "metadata": {
        "id": "Kd6ozx_YkCO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.unique(y_train_tensor)"
      ],
      "metadata": {
        "id": "wVR1z5MLkHxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "kdthNUy3kLNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentClassifier(\n",
        "        d_model=D_MODEL,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        num_heads=NUM_HEADS,\n",
        "        drop_prob=DROP_PROB,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        num_classes=NUM_CLASSES\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "1Va0UX9VkK36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "c47i5NgLldhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Starting Training...\")\n",
        "\n",
        "# --- Step 4: The Training Loop ---\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # --- Loop over batches ---\n",
        "    for batch_inputs, batch_labels in train_loader:\n",
        "        # Move data to the selected device (GPU/CPU)\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        outputs = model(batch_inputs)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # 4. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- Track statistics ---\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += batch_labels.size(0)\n",
        "        correct_predictions += (predicted == batch_labels).sum().item()\n",
        "\n",
        "    # --- End of Epoch ---\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = (correct_predictions / total_samples) * 100\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "          f\"Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\n Training Finished!\")"
      ],
      "metadata": {
        "id": "SBd89bo0lj5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embedding(sentence, embedding_model):\n",
        "    \"\"\"\n",
        "    Converts a raw sentence into a 384-dimension numerical vector\n",
        "    using a Sentence Transformer model.\n",
        "    \"\"\"\n",
        "    # The model.encode() method returns a NumPy array.\n",
        "    embedding = embedding_model.encode(sentence)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "Dhu3s-RZlono"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, classifier_model, embedding_model, class_names, device):\n",
        "    \"\"\"\n",
        "    Predicts the sentiment of a given text using the trained model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input comment to analyze.\n",
        "        classifier_model (nn.Module): Your trained SentimentClassifier model.\n",
        "        embedding_model (SentenceTransformer): The model to convert text to vectors.\n",
        "        class_names (dict): A dictionary mapping class indices to names (e.g., {0: 'Negative'}).\n",
        "        device (str): The device to run inference on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted class name (str) and the\n",
        "               confidence score (float).\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode (disables dropout, etc.)\n",
        "    classifier_model.eval()\n",
        "\n",
        "    # 1. PREPROCESS: Convert the text to its numerical representation.\n",
        "    embedding = get_sentence_embedding(text, embedding_model)\n",
        "\n",
        "    # 2. FORMAT FOR PYTORCH:\n",
        "    #    a) Convert numpy array to a PyTorch tensor.\n",
        "    #    b) Add a batch dimension (shape [384] -> [1, 384]).\n",
        "    #    c) Move the tensor to the correct device.\n",
        "    input_tensor = torch.tensor(embedding).unsqueeze(0).to(device)\n",
        "\n",
        "    # 3. PREDICT:\n",
        "    #    Perform prediction without calculating gradients to save memory and speed.\n",
        "    with torch.no_grad():\n",
        "        # Get the model's raw output scores (logits).\n",
        "        logits = classifier_model(input_tensor)\n",
        "\n",
        "    # 4. INTERPRET:\n",
        "    #    a) Convert raw logits to probabilities using the softmax function.\n",
        "    #    b) Find the highest probability and its corresponding class index.\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    confidence, predicted_class_idx = torch.max(probabilities, dim=1)\n",
        "\n",
        "    # Map the predicted index to its human-readable class name.\n",
        "    predicted_class_name = class_names[predicted_class_idx.item()]\n",
        "\n",
        "    # Return the final result.\n",
        "    return predicted_class_name, confidence.item()\n"
      ],
      "metadata": {
        "id": "n3KssLfYoAAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Positive', 'Neutral', 'Negative', 'Irrelevant'"
      ],
      "metadata": {
        "id": "GO5U0k64otaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "CLASS_NAMES = {2: 'Negative', 1: 'Neutral', 0: 'Positive',3:'Irrelevant'}\n",
        "test_comments = [\n",
        "        \"that was the first borderlands session in a long time where i actually had a really satisfying combat experience. i got some really good kills\",\n",
        "        \"im getting on borderlands and i will kill you all,\",\n",
        "        \"Man Gearbox really needs to fix this dissapointing drops in the completely new Borderlands 3 Days DLC i cant e be fine having to be farm bosses on Mayhem 10 to e get 1 legendary foot drop while anywhere else i get 6 - 10 drops.. It Really sucks to alot\",\n",
        "        \"<unk> Gearbox really time to fix this 10 drops in the new Borderlands 3 DLC or be fine to force bosses on Mayhem 10 to get a legendary drop while everyone else i get 6-10 drops.. Really needs alot\"\n",
        "    ]\n",
        "print(\"\\n--- Running Predictions ---\")\n",
        "for comment in test_comments:\n",
        "  # This now uses your actual trained model.\n",
        "  sentiment, confidence = predict_sentiment(comment, model, embedding_model, CLASS_NAMES, device)\n",
        "\n",
        "  print(f\"Comment: '{comment}'\")\n",
        "  print(f\"--> Predicted Sentiment: {sentiment} (Confidence: {confidence:.2%})\\n\")"
      ],
      "metadata": {
        "id": "doExv2d_pB3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "g2T8i802rM0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HQ7i9Dh2u7R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_PATH = '/content/drive/MyDrive/encoder_model'\n",
        "if not os.path.exists(GDRIVE_PATH):\n",
        "    os.makedirs(GDRIVE_PATH)\n",
        "MODEL_SAVE_PATH = os.path.join(GDRIVE_PATH, 'sentiment_model.pth')\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Model saved successfully to: {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "Znlj-XyPvACT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_load = SentimentClassifier(\n",
        "        d_model=D_MODEL,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        num_heads=NUM_HEADS,\n",
        "        drop_prob=DROP_PROB,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        num_classes=NUM_CLASSES\n",
        "    )"
      ],
      "metadata": {
        "id": "trdpyY49wF0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_to_load = SentimentClassifier()\n",
        "\n",
        "# 2. Load the saved weights from your Google Drive\n",
        "#    The MODEL_SAVE_PATH is the same as defined above\n",
        "model_to_load.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# 3. Set the model to evaluation mode\n",
        "#    This is important for inference as it disables layers like Dropout\n",
        "model_to_load.eval()\n",
        "\n",
        "print(\"\\nModel loaded successfully and is ready for prediction!\")"
      ],
      "metadata": {
        "id": "NcilRul_vs23"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}